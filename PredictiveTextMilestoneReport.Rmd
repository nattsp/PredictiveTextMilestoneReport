---
title: "Predictive Text - Milestone Report"
author: "Natalie Phillips"
date: "29 October 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Capstone Project for Data Science Specialization

## First Milestone Report

Text prediction is an often seen feature on our mobile phones and in our web browsers. Text prediction can assist us to type faster on our phones and correct our spelling when we make a mistake. SwiftKey our partner in this Capstone project boast over over 250 million users of their keyboard for mobile phones.

## Corpora to train our own models

We have been provided with a number of text corpus to develop and train our own text prediction Model. The corpus cover a number of languages:

* English
* German
* Russian
* Finnish

and a number of sources:

* Twitter
* news
* blogs.

I have downloaded the text corpus and started examining with the English language files.

## English text examples

The text examples vary in size in both number of entries and length of the entries. This is not surprising given that Twitter has historically had a character limit of 140.  The text files have the following specifications:

```{r echo=FALSE}
load(file = "..\\..\\Data\\filesDF.RData")
library(knitr)
library(ggplot2)
library(tidytext)
library(dplyr)
```

```{r echo = FALSE, results='asis'}
kable(filesDF, caption = "Size of English text corpus")
```

### Sample the data

So far my efforts have been around cleaning the data and getting a feel for what is in the text. I would like to recognise the online text book "Text Mining with R - A Tidy Approach" by Julia Silge and David Robinson which can be found at: <http://tidytextmining.com>. This book along with other resources provided through the course have taught me much in the way of string manipulation and I have loosly followed the steps they did.

To start with I sampled around 10% of the US Twitter data using random sample generated by flipping a randon coin using 

```
flips <- rbinom(totalNumLines, size = 1, prob = 0.1)
```

to generate a series of 0s and 1s to decide which lines of text to keep. The sample had 236,520 different text examples.

In contrast when sampling the blogs and news data I calculated the probability to yield samples of roughly the same size.

### Clean the data

I have performed several levels of cleaning so far including:

* removing numbers
* removing non-word characters
* converting to lowercase.

```
library(tidytext)
sampleNewsTidy <- sampleNewsT %>%
    mutate(text = str_replace_all(text, '\\d', ' ')) %>%
    mutate(text = str_replace_all(text, '[^\\w ]', ' ')) %>%
    mutate(text = tolower(text))
```

The texts were then converted into a "bag of words" and counted across the sample. 

```
sampleNewsWords <- sampleNewsTidy %>%
    unnest_tokens(word, text) %>%
    mutate(sample = "News") %>%
    select(sample, word) %>%
    count(sample, word, sort = TRUE) %>%
    ungroup()
```

As you would expect the most frequent words are the least interesting as can be see in the header of the sampleNewsWords dataframe/tibble.

```{r echo=FALSE}
library(tidytext)
load(file = "..\\..\\Data\\sampleNewsWords.RData")
```

```{r}
print(sampleNewsWords)
```

These frequent words may need to be taken out as they provide little value in determining what a text is about. They will need to be left in when we get to text prediction however as they are needed to make properly formed sentences.

### Interesting compareson between data sets

I wanted to see how the text from the three different sources compared to each other. I labeled each set and combined them together. Then I used the idea that important words are common but not too common. To calculate this importance I used the bind_tf_idf function in the tidytext package. I won't show the calculation however the graph of the results for each of the texts is interesting.

```{r echo=FALSE, message=FALSE}
load(file = "..\\..\\Data\\sampleWords_idf.RData")

plot_important_words <- sampleWords_idf %>%
    arrange(desc(tf_idf)) %>%
    mutate(word = factor(word, levels = rev(unique(word))))


plot_important_words %>% 
    group_by(sample) %>% 
    top_n(15) %>% 
    ungroup %>%
    ggplot(aes(word, tf_idf, fill = sample)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~sample, ncol = 2, scales = "free") +
    coord_flip()

```

Several thinks immediately jump out and I feel this is a lot more instructive than the previous list of words which were essentially "stop words".

We can see look at the "important" words in each type of text that:

* blogs and news have similar sets of words
* twitter uses shorter words
* twitter uses more abbreviations
* twitter has moree:
  + slang words
  + swear words
  + potentially racest language.
  
These results suggest to me further cleaning I need to perform in the data set. Further data cleaning improvements could be:

* investigate unexpected words such as city√¢
* remove swear words
* remove special characters that do not form real words
* remove frequently used words.

### Looking ahead to the prediction algorithm

Getting use to working with and manipulating the text has been a learning curve. With the prediction algorithm I feel I have more questions than answers.

Ultimately using the knowledge I have gained so far I would like to:

- make a context away predict algorithm with different word suggesions for a tweet to a blog
- keep in the "stop words" and full version of the words have accurate word predictions
- optimise speed by making my database of word combinations small so cutting which I imagine can be done leaving out low frequency word combinations.

My questions are numerious and include:

- can the dictionary I'm developing be put to good use
- should I continue using tidytext and tibbles
- does the tm package have better routines and functions
- is it worth trying to perform word tokenisation on hash tags?

I look forward to discoverying more. Please suggest good resources where I can expand my knowledge and thinking. I welcome all and any suggestions.
